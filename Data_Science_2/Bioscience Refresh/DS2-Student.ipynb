{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploration: Linear Regression and Classification\n",
        "\n",
        "A fundamental component of mastering data science concepts is applying and practicing them. This exploratory notebook is designed to provide you with a semi-directed space to do just that with the Python, linear regression, and ML-based classification skills that you either covered in an in-person workshop or through Microsoft Learn. The specific examples in this notebook apply NumPy and Pandas concepts in a life-sciences context, but they are applicable across disciplines and industry verticals.\n",
        "\n",
        "This notebook is divided into different stages of exploration. Initial suggestions for exploration are more structured than later ones and can provide some additional concepts and skills for tackling data-science challenges with real-world data. However, this notebook is designed to provide you with a launchpad for your personal experimentation with data science, so feel free to add cells and running your own experiments beyond those suggested here. That is the power and the purpose of a platform like Jupyter notebooks!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Refresher on Notebooks\n",
        "\n",
        "Before we begin, you will need to important the principal libraries used to explore and manipulate data in Python: NumPy, Pandas, and Scikit-learn. The cell below also imports Matplotlib, the main visualization library in Python. For simplicity and consistency with prior instruction, industry-standard aliases are applied to these imported libraries. The cell below also runs `%matplotlib inline` magic command, which instructs Jupyter to display Matplotlib output directly in the notebook. This cell also imports many of the specific functions from Scikit-learn that you will need, but feel free to import others as you see fit in the course of your exploration."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "%matplotlib inline\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As it might have been a while since you last worked with Jupyter notebooks, here is a quick refresher on efficiently using them.\n",
        "\n",
        "### Notebook Cells\n",
        "\n",
        "Notebook cells are divided into Markdown text cells and interactive code cells. You can easily recognize code cells by the `In [ ]:` to the left of them.\n",
        "\n",
        "Code in a code cells has only been executed -- and is thus available for use in other code cells in the notebook -- if there is a number beside the code cell (for example, `In [1]:`).\n",
        "\n",
        "To run the code in a cell, you can click the **Run** icon at the top left of a code cell or press **`Ctrl` + `Enter`**.\n",
        "\n",
        "Cells in Jupyter notebooks can be in one of two modes: edit or command. The active cell (the cell that you have clicked on or navigated to) is in edit mode if it has a green border around it. Code cells are in edit mode by default when you click on them. (If, however, you navigate to a code cell by other means, it will be in command mode; simply press **`Enter`** to change it to edit mode.)\n",
        "\n",
        "An active cell with a blue border is in command mode. You can change from edit mode to command mode in a cell by pressing **`Esc`** or by running the contents of a cell. Running a Markdown cell will render the Markdown.\n",
        "\n",
        "To add cells to a notebook, select **Insert cell** from the menu at the top of the screen or in the margin to the left of a cell in edit mode. You can delete unneeded cells by clicking the **Delete cell** icon at the top right of the cell.\n",
        "\n",
        "Entering command mode enables you to use keyboard shortcuts to quickly work with your notebook. Here are some of the most useful ones:\n",
        " - Add cell above: **`A`**\n",
        " - Add cell below: **`B`**\n",
        " - Delete cell: **`D`**,**`D`** (press **`D`** twice)\n",
        "\n",
        "Cheatography has a good compendium of additional keyboard shortcuts [here](https://cheatography.com/weidadeyue/cheat-sheets/jupyter-notebook/).\n",
        "\n",
        "You can run code cells from edit mode or command mode.\n",
        "\n",
        "### Documentation and Help\n",
        "\n",
        "Documentation for Python objects and functions is available directly in Jupyter notebooks. In order to access the documentation, simply put a question mark in front of the object or function in a code cell and execute the cell (for example, `?print`). A window containing the documentation will then open at the bottom of the notebook.\n",
        "\n",
        "On to exploration!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Guided Exploration\n",
        "\n",
        "The first dataset provided with this notebook is `mammals.csv`, which documents the body weight (in kilograms) and the brain weight (in grams) of 62 mammals.  (Source: Rogel-Salazar, Jesus (2015): Mammals Dataset. figshare. Dataset. https://doi.org/10.6084/m9.figshare.1565651.v1. Drawn from Allison, T. and Cicchetti, D. V. (1976). Sleep in mammals: ecological and constitutional correlates. *Science, 194*, 732â€“734.)\n",
        "\n",
        "### Import and Investigate the Data\n",
        "\n",
        "Use `pd.read_csv()` to import `mammals.csv` and perform any other initial investigation you feel necessary in order to become familiar with the dataset. (For a refresher on importing data into Pandas, see the Reactors modules on Manipulating and Cleaning Data or Pandas or refer to the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot the Data\n",
        "\n",
        "Often the best way to get a sense of your data is to do so visually. A scatter plot would be most appropriate for this dataset; Pandas DataFrames have [two](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html) [methods](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.scatter.html) that can be used to create scatter plots."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Don't worry: your scatter plot should be hard to read. Most of the values are clustered at the tiny end with two very large mammals in particular (the Asian and African elephants) skewing the scale."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transform Your Data\n",
        "\n",
        "Because of the decidedly non-linear dispersion of mammalian size, you will need to transform you data in order to more clearly see the relationships in it.\n",
        "\n",
        "**Group or Partner Discussion**\n",
        "\n",
        " - Which feature in the dataset should you transform? Should you transform both? In either case, why? What do you predict you might see after the transformation?\n",
        " - What transformation do you think you should use? What key words do you see in the documentation for the Pandas plot method that might give a hint?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit and Plot a Linear Regression on Your Data\n",
        "\n",
        "Transformed, the data presents an elegant linear relationship. How tight is it? Fit and plot a simple linear regression model for the data to find out. If you are unsure about how to do this, refer to the Reactors module on Machine Learning Models for a reminder. You will need to perform the following steps to do fit the model:\n",
        "\n",
        "1. Split your dataset into predictor variable (`X` is a common name of this variable) and the respons variable (`y` is a common variable name for this). (Remember to transform your data at this stage in the same way that you transformed it when you plotted it.)\n",
        "2. Further divide your into training and test subsets. (There is a Scikit-learn function for this.)\n",
        "3. Create the linear regression model object.\n",
        "4. Fit the model to the training data.\n",
        "\n",
        "**Note:** You will get an error when you try to fit the data. Refer to https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.to_numpy.html and to https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.reshape.html for documenation on how to reshape the your data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now plot your model with your data. Run [`matplotlib.pyplot.plot`](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.plot.html) along with your transformed scatter plot of the data to do so. (Using a different color for the model will help you see it more clearly.)\n",
        "\n",
        "**Group or Partner Discussion**\n",
        "\n",
        " - Does your model plot as you expected it to? If not, why? What transformations do you need to run on it to produce the plot you expect?\n",
        "\n",
        "**Note:** Python has functions that can perform the necessary transformations and the dataset is small enough that you will likely not notice a lag in performance by using the native Python functions. That said, it is a good habit to develop to use the NumPy ufuncs for when you deal with larger datasets. For a reminder on these, refer to the Reactors moduls on NumPy."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your transformations were successful, you should see a gratifyingly tight line for your data. But while it may look good, how good is it actually? Us the [$R^2$ score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) to find out."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Intermediate Exploration\n",
        "\n",
        "Now that you know more about the mammals dataset, the next step is to start asking more difficult questions about it. For example, the $R^2$ score you got for your linear model on it was probably quite good, but how good is 'good' in reality? Plot the linear model against the untransformed data to see this more clearly.\n",
        "\n",
        "To create this plot, you will again use the Pandas `plot` (or `plot.scatter`) method in conjunction with `matplotlib.pyplot.plot`. However, you will need to create Numpy array to supply inputs along the x-axis for `matplotlib.pyplot.plot`. (See the Reactors Numpy module for a refresher on how to do that.) You will also need to transform that array when you input it to you model and further transform the output of your model to plot it accurately.\n",
        "\n",
        "**Group or Partner Discussion**\n",
        "\n",
        " - What errors are you getting? Do you need to reshape any of the data? Are zero values causing trouble for any of your transformations? How should you best deal with those?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** Another way to approach this challenge is to deal with the model coefficient and intercept directly. Recall that linear models take the form of `Y`$ = $`intercept`$ + $`coefficient`$ * $`x`. Check the documentation for your model object to see how to access those values in your model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### When Linear Regression is Less Helpful\n",
        "\n",
        "Now consider a case where linear regression might not provide the help that you would like. To investigate this, import and plot the `lynx.csv` dataset, which contains annual numbers of lynx trappings for 1821â€“1934 in the Mackenzie River area of Canada. (Source: Campbell, M. J. and Walker, A. M. (1977). A Survey of statistical work on the Mackenzie River series of annual Canadian lynx trappings for the years 1821â€“1934 and a new analysis. *Journal of the Royal Statistical Society series A, 140*, 411â€“431. doi: [10.2307/2345277](http://doi.org/10.2307/2345277).)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your plot should show and interesting cyclical pattern to the data. A high peak in lynx numbers is followed by three smaller peaks every 9-10 years and then the pattern repeats itself. Ample food supply enables Mackenzie River lynx to reproduce to high numbers, after which the population plummets due to lack of food. The food supply gradually builds back up, enabling a repeat of the boom-and-bust population growth of the lynx.\n",
        "\n",
        "**Group or Partner Discussion**\n",
        "\n",
        " - Is a linear model appropriate for data like this? Why or why not? What do you suspect you might see if you attempt to fit a linear model to this data?\n",
        "\n",
        "Go ahead and fit and plot this data as you did for the mammals dataset above."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now check the $R^2$ score for this model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Group or Partner Discussion**\n",
        "\n",
        " - What does this $R^2$ score mean? What interpretation might it carry? (This [essay](https://web.maths.unsw.edu.au/~adelle/Garvan/Assays/GoodnessOfFit.html) from the University of New South Wales might help your discussion.)\n",
        " - Even if your $R^2$ score cannot explain the proportion of variance explained by your model, what information might it nonetheless provide?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classification\n",
        "\n",
        "In order to explore ML-based classification, let's return to a dataset you already encountered in the Reactors Manipulating and Cleaning Data module. Import the Python scikit-learn library and use an iconic dataset that every data scientist has seen hundreds of times: British biologist Ronald Fisher's *Iris* data set used in his 1936 paper \"The use of multiple measurements in taxonomic problems.\"\n",
        "\n",
        "You have already imported the Scikit-learn library containing the `iris` dataset; you can access it using the `load_iris()` function. (Look at the `?load_iris` documentation for more information about this function; the data and target information is stored separately.) You might also find it helpful to create a DataFrame with the iris information in order to investigate it. (Check the Reactors Pandas module for a refresher on how to load data into DataFrames.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the Reactors Machine Learning module, you used the logistic regression and decision tree algorithms to classify observations into two categories, but there are many other kinds of classification algorithms that you will explore in this module.\n",
        "\n",
        "#### $K$-means Clustering\n",
        "\n",
        "$K$-means clustering is an example of unsupervised machine learning. Rather than having to train a model, the $k$-means algorithm examines all of the data to make a determination of which category to assign a particular observation. All that you have to do is supply the algorithm with the number of categories into which you want observations classified.\n",
        "\n",
        "Based on the number of species in the `iris` dataset, what is the most appropriate number of clusters to submit to the algorithm? Fit a $k$-means model for that number of clusters and measure its accuracy. (Consult the ?metrics.accuracy_score documentation for information on how to do this.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Group or Partner Discussion**\n",
        "\n",
        " - Is this accuracy surprising? Try different values for the `random_state` parameter in the `KMeans` function (such as 0, 1, 2). Why the large disparities in accuracy based on the random state of the algorithm? (In the Individual Exploration section below, you can explore some of the structure of the `iris` dataset that can help generate these disparities.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $K$-nearest Neighbors\n",
        "\n",
        "A classification algorithm that might work better on the `iris` dataset is the $k$-nearest neighbors algorithm (abbreviated $k$-NN). It works by comparing an observation to its $k$ nearest training observations in feature space (where $k$ is a parameter supplied by the user). $k$-NN is a supervised algorithm, so you will need to split the data into training and test subsets. Fit a $k$-NN model and use `metrics.accuracy_score` to examine its accuracy."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Group or Partner Discussion**\n",
        "\n",
        " - Try supplying different `random_state` parameters to the `train_test_split` function (such as `random_state=0` and `random_state=2`). What causes the differences in accuracy?\n",
        "\n",
        "One way to mitigate the luck of the draw inherent in training/test splitting is to do so repeatedly. Testing your models in this way is called cross validation (or $k$-fold cross validation after the number of times you resplit the data, the folds). Scikit-learn has a [good page](https://scikit-learn.org/stable/modules/cross_validation.html) in its documentation on the concept.\n",
        "\n",
        "Use the `cross_val_score` model to perform a 10-fold cross validation on your $k$-NN model and take the mean of the accuracy scores."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Individual Exploration\n",
        "\n",
        "Here are some ideas to continue your exploration of classification and predictive ML algorithms:\n",
        "\n",
        "- What number of nearest neighbors provides the highest average accuracy for your $k$-NN model? Try referring back to the Reactors Python module for a refresher on loops and data structures to see how you could automate this comparison over many several different values of $k$."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Visualize the `iris` dataset to see why the $k$-means algorithm produced the accuracy that it did.\n",
        " \n",
        " The `iris` dataset has four features, which means that a true scatter plot of all of the observations in it would require four dimensions, which is impossible to visualize directly. However, you can use a technique called principle component analysis (PCA) to reduce this to three dimensions with minimal loss of information. (Don't worry about the details of how PCA works; you will cover it in another session in the Reactors PCA module.)\n",
        " \n",
        " To visualized a \"flattened\" `iris` dataset, you will need to fit a PCA transformation with the data from the dataset. You can get some ideas about what code to use from this [page](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) from the Scikit-learn documentation."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this visualization in hand, why might $k$-means clustering not be a good classification algorithm to use with the `iris` dataset? (This [Wikipedia article](https://en.wikipedia.org/wiki/K-means_clustering) on $k$-means clustering might help your discussion.)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3-final",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}