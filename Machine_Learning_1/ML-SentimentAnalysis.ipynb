{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning With Text : Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first cut at a series of exercises to conduct sentiment analysis for the Microsoft Reactor Machine Learning initiative. Machine learning does not just have to be about numbers. We can turn text into numbers and look at things like document similarity, clusters, etc. We can also engage Natural Language Processing (NLP) tools to perform sentiment analysis, named entity recognition and more.\n",
    "\n",
    "Consider the explosion of data we face and then realize that we have similar issues with documents in the form of news stories or magazine articles. You either need to read very fast (or constantly) or rely on some reporting source to tell you what is going on. It is clear that not every news story is unbiased and sometimes there may be positive or negative spin on a story, so how do you look behind the scenes to see what is going on?\n",
    "\n",
    "We can increasingly rely on machine learning systems to help us process documents and extract information and other value from them. If you work in a particular industry, you could start to see what is being said about your industry. If you have an investment portfolio, you could imagine letting software read the news and draw your attention to positive or negative swings in reporting. If you work for an organization, you could get a sense of what people were saying about you on social media.\n",
    "\n",
    "In our exercise, we will use an RSS feed of the news headlines from Microsoft Money as a source of data. As time passes, you could store and assess what is the general sentiment of the news? Who is being mentioned most often? Are positive and negative things being said about these individuals, organizations or places?\n",
    "\n",
    "PLEASE NOTE: THE RESULTS OF THIS EXERCISE WILL CHANGE OVER TIME AND DO NOT CONSTITUTE LEGAL OR FINANCIAL ADVICE. PLEASE CONSULT WITH YOUR OWN EXPERTS BEFORE MAKING ANY DECISIONS ON WHAT YOU READ IN THE NEWS OR SEE HERE.\n",
    "\n",
    "You will need to install BeautifulSoup, nltk and feedparser.This is the first cut at a series of exercises to conduct sentiment analysis for the Microsoft Reactor Machine Learning initiative. Machine learning does not just have to be about numbers. We can turn text into numbers and look at things like document similarity, clusters, etc. We can also engage Natural Language Processing (NLP) tools to perform sentiment analysis, named entity recognition and more.\n",
    "\n",
    "Consider the explosion of data we face and then realize that we have similar issues with documents in the form of news stories or magazine articles. You either need to read very fast (or constantly) or rely on some reporting source to tell you what is going on. It is clear that not every news story is unbiased and sometimes there may be positive or negative spin on a story, so how do you look behind the scenes to see what is going on?\n",
    "\n",
    "We can increasingly rely on machine learning systems to help us process documents and extract information and other value from them. If you work in a particular industry, you could start to see what is being said about your industry. If you have an investment portfolio, you could imagine letting software read the news and draw your attention to positive or negative swings in reporting. If you work for an organization, you could get a sense of what people were saying about you on social media.\n",
    "\n",
    "In our exercise, we will use an RSS feed of the news headlines from Microsoft Money as a source of data. As time passes, you could store and assess what is the general sentiment of the news? Who is being mentioned most often? Are positive and negative things being said about these individuals, organizations or places?\n",
    "\n",
    "PLEASE NOTE: THE RESULTS OF THIS EXERCISE WILL CHANGE OVER TIME AND DO NOT CONSTITUTE LEGAL OR FINANCIAL ADVICE. PLEASE CONSULT WITH YOUR OWN EXPERTS BEFORE MAKING ANY DECISIONS ON WHAT YOU READ IN THE NEWS OR SEE HERE.\n",
    "\n",
    "You will need to install BeautifulSoup, nltk and feedparser if they are not installed.\n",
    "\n",
    "To install them, you can type:\n",
    "\n",
    "```pip install bs4 nltk feedparser```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /Users/brian/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /Users/brian/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/brian/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 352
    }
   ],
   "source": [
    "import collections\n",
    "import string\n",
    "import feedparser\n",
    "import requests\n",
    "import hashlib\n",
    "import tempfile\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "\n",
    "# We need to load the Stopwords and the lexicons that the VADER algorithm uses to assess sentiment.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first method is going to create a directory for us to cache our results in so we do not have to keep downloading them over and over. We will use the OS definition of where temporary directories should go, and then we will create a subdirectory for our purposes called `msft-reactor-ml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTempDirIfNecessary() :\n",
    "    tempDir = tempfile.gettempdir()\n",
    "    path = os.path.join(tempDir, \"msft-reactor-ml\")\n",
    "\n",
    "    if(not os.path.exists(path)) :\n",
    "        os.mkdir(path)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because during development, you might want to run your code many times over fairly large collections of documents, you don't want to be a bad citizen and pound backend servers with requests. So, we add the following function to store files after downloading them. We hash the URL to a consistent name so if we go looking for it in the future, we can just grab the local copy.\n",
    "\n",
    "*Note*: This approach is only triggering off the location identity for the documents, not the contents. As a homework assignment, you might improve this function to use cache-controls from the server or check to see if the timestamp of the file is older than a particular age. If it is, you might ditch the cached copy to fetch a newer one. That's mostly going to be useful for URL results that change over time like RSS feeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchUrlIfNecessary(url) :\n",
    "    # Initialize the temporary directory if we need to\n",
    "    path = createTempDirIfNecessary()\n",
    "\n",
    "    # Hash a normalized version of the URL using a SHA-256\n",
    "    # hashing function.\n",
    "    hashed = hashlib.sha256(url.encode()).hexdigest()\n",
    "\n",
    "    # The raw file will be stored as <HASH>.in.txt to differentiate\n",
    "    # it from a cleaned up or processed version as we will see later.\n",
    "    file = os.path.join(path, f\"{hashed}.in.txt\")\n",
    "\n",
    "    # Once we know what the associated file with the URL should be called\n",
    "    # (via its hash, not URL), we will see if it exists. If it doesn't, we\n",
    "    # will store it.\n",
    "    if(not os.path.exists(file)) :\n",
    "        response = requests.get(url)\n",
    "        with open(file, mode='wb') as localfile:\n",
    "            localfile.write(response.content)\n",
    "\n",
    "    # Whether the file existed before or not, we return the hashed name of \n",
    "    # the file so analytical code can just open it up as need be.\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method was inspired by the StackOverflow entry in the link provided. When you are extracting text from structured documents such as HTML, there are several elements that will just be noise such as JavaScript script elements, stylesheet elements, etc. A given source may require additional handling so for any particular site that you want to scrape from, you may need to parameterize the elements to remove.\n",
    "\n",
    "For convenience, we add the ability to specify a list of DOM element classes and ids to remove as well. You will see an example of that being used below. To start off with, however, you can just pass in the HTML file as a string and BeautifulSoup will do most of the hard work for us. \n",
    "\n",
    "*Note:* As a homework assignment, refactor this method to accept the HTML elements as a default parameter so they can be overridden as needed by client code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/30565404/remove-all-style-scripts-and-html-tags-from-an-html-page/30565420\n",
    "\n",
    "def cleanMe(html, class_filters=[], id_filters=[]):\n",
    "    soup = BeautifulSoup(html, \"html.parser\") # create a new bs4 object from the html data loaded\n",
    "    \n",
    "    # Remove all javascript and stylesheet code\n",
    "    for script in soup([\"script\", \"style\", \"a\", \"li\", \"noscript\", \"span\", \"meta\"]): \n",
    "        script.extract()\n",
    "\n",
    "    # Remove any elements that have any of the specified class identifiers.\n",
    "    # There could be several instances.\n",
    "    for c in class_filters:\n",
    "        elems = soup.find_all(class_=c)\n",
    "        for elem in elems:\n",
    "            elem.decompose()\n",
    "\n",
    "    # Remove any elements that have any of the specified element identifiers.\n",
    "    # There should only be one instance per id.\n",
    "    for i in id_filters:\n",
    "        elem = soup.find(id=i).decompose()\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method retrieves the local copy of a file associated with a URL and then processes it prior to analysis.\n",
    "\n",
    "*INSTRUCTOR NOTE*: If you want the students to spend some time, have them pick a new data source and figure out which id filters and class filters should be removed to improve the quality of the results. The values below are useful to remove from the BBC RSS feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchAndProcessUrlIfNecessary(url) :\n",
    "    entryFile = fetchUrlIfNecessary(url)\n",
    "\n",
    "    # The value returned from the fetchUrlIfNecessary function will\n",
    "    # be <HASH>.in.txt so we grab the <HASH> part and indicate that\n",
    "    # this is the processed version.\n",
    "    processedFile = f\"{entryFile[:-7]}.out.txt\"\n",
    "\n",
    "    # If the file doesn't exist, we will process the raw file.\n",
    "    if(not os.path.exists(processedFile)) :\n",
    "        with open (entryFile, \"r\" ) as myfile:\n",
    "            data = myfile.read()\n",
    "            data = cleanMe(data,\n",
    "                id_filters=[\n",
    "                  'orb-header', \n",
    "                  'core-navigation'\n",
    "                ],\n",
    "                class_filters=[\n",
    "                  'off-screen', \n",
    "                  'tags-container', \n",
    "                  'faux-block-link',\n",
    "                  'distinct-component-group',\n",
    "                  'orb-footer', \n",
    "                  'share', \n",
    "                  'column--secondary', \n",
    "                  'more-on-this-story',\n",
    "                  'story-body__h1',\n",
    "                  'player-with-placeholder',\n",
    "                  'vxp-media-player-component',\n",
    "                  'vxp-related-content-component'\n",
    "                ])\n",
    "\n",
    "            # We will store the result and close\n",
    "            # up our files.\n",
    "            outfile = open(processedFile, \"w\")\n",
    "            outfile.write(data)\n",
    "            outfile.close()\n",
    "            myfile.close()\n",
    "    else:\n",
    "        # If the processed file exists, we will just return\n",
    "        # the results for further analysis.\n",
    "        with open (processedFile, \"r\" ) as myfile:\n",
    "            data = myfile.read()\n",
    "            myfile.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is simply a place to experiment with the fetchAndProcessUrlIfNecessary() function. As you experiment with the previous functions, you can see what the resulting document looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Coronavirus: Can China test all of Wuhan in only 10 days? - BBC News\nChina is drawing up ambitious plans to test the entire population of Wuhan, the city where the Covid-19 pandemic began.The announcement came after the emergence of six new coronavirus cases in the city - the first ones since early April.The authorities had originally promised to test all 11 million people in 10 days.But it now appears they might be aiming for a less ambitious timetable.How long will the testing take?In late April, the Hubei provincial government reported 63,000 people were being tested in Wuhan every day.\nAnd by 10 May, that figure had dropped to just under 40,000.There are more than 60 testing centres across the city, according to the official Hubei Daily newspaper. These have a maximum capacity of 100,000 tests a day at most, making it hard to see how a target of testing the entire population in just 10 days could be met.So the authorities have indicated the tests will not all start and finish within the same 10-day period. \"Some districts [in the city] will start from 12 May, others from 17 May, for example,\" the Wuhan Centre for Disease Control said.\"Each district finishes its tests within 10 days from the date it started.\" And\npreparations for carrying out tests had begun in two out of the city's 13 districts.What proportion have been tested already?The authorities say they have now tested more than three million people in the city.Wuhan University pathogen biology department deputy director Yang Zhanqiu told the Global Times newspaper he believed up to five million Wuhan residents may have already been tested.The population of the city - originally 11 million - has also fluctuated over time.The authorities said up to five million people had left the city for the lunar New Year holiday before it was locked down on 23 January.The lockdown then lasted until 8 April, but it is unclear how many of these residents have now returned.Should everyone be tested?Wuhan University's Yang Zhanqiu said there was no need to test everyone living in neighbourhoods with no reported cases.\nThe authorities have said they will begin with people considered most at risk - for example in the older, more densely populated areas, as well as those in key jobs such as healthcare.Also, people who have been tested in the previous seven days will not need to be tested again.But Chinese Center for Disease Control and Prevention chief epidemiologist Wu Zunyou told state TV: \"The virus could take longer to manifest itself in patients with weak immunity and these people are also prone to 'on' and 'off' symptoms.\"\nYang Zhanqiu adds: \"You'll never know if people were infected after testing negative.\" And US-based Council on Foreign Relations senior fellow for global health Huang Yanzhong said: “There would still be the possibility of isolated outbreaks in the future, which even large-scale testing will not address.\"Additional reporting and research by Yitsing Wang, in Beijing, and Pablo Uchoa and Wanyuan Song, in London.\n"
    }
   ],
   "source": [
    "parsedFile = fetchAndProcessUrlIfNecessary(\"https://www.bbc.co.uk/news/world-asia-china-52651651\")\n",
    "print(parsedFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is intended to be modified during an exercise. Without the clean up steps, the results will suffer. Once you try it a few times without the clean up steps, you should remove some of the comments and lowercase the words,\n",
    "remove punctuation and stopwords, stem the tokens, etc. and see how the analysis improves down below.\n",
    "\n",
    "*INSTRUCTOR NOTE:* Make sure the students know what they are doing and which of the comments need to go together. It's probably more interesting and useful if they don't uncomment everything all at once, but for example, you need to uncomment the stop_words set creation AND the if clause to remove the stopwords. Remind the students to pass in True for the filterStopWords parameter to activate that behavior in the tokenization process. It's left to you which lines to start commented out depending on time and course configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenizeText(text, filterStopWords=False) :\n",
    "   #stem = nltk.stem.SnowballStemmer('english') \n",
    "   text = text.lower()\n",
    "   stop_words = set(stopwords.words('english'))\n",
    "\n",
    "   for token in nltk.word_tokenize(text):\n",
    "       if token in string.punctuation: continue\n",
    "       if filterStopWords and token in stop_words: continue\n",
    "       yield token # stem.stem(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is intended as an exercise if you have time. We want you to count how many of each word shows up in a text. You will need to tokenize the text with the previous method. As you make changes to the body of the previous method, you should see different results. What happens if you don't filter out the stopwords? What if you don't lower case the text?\n",
    "\n",
    "*INSTRUCTOR NOTE*: Leave the function definition as it is but maybe remove the body. The students should be told to tokenize the text with the method above and iterate over the results. If they don't filter stop words, then common words will show up as the most common words. If they don't lowercase the text above, case differences will show up as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordFrequency(text, filterStopWords=False) :\n",
    "   dict = collections.Counter()\n",
    "   for token in tokenizeText(text, filterStopWords):\n",
    "       dict[token] += 1\n",
    "   return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is intended as an exercise if you have the time. We want you to tokenize the text as sentences and then analyze each one for its sentiment. Return a list with each sentence and its scores. Keep in mind you can add the sentence to the results from the NLTK Sentiment analyzer.\n",
    "\n",
    "*INSTRUCTOR NOTE:* Leave the function definition as is but maybe remove the body. The students should be told to tokenize the text as sentences run them through the NLTK SentimentIntensityAnalyzer. The return value should be a list of dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeSentiment(text):\n",
    "    results = []\n",
    "\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    for sentence in sentences:\n",
    "        scores = sid.polarity_scores(sentence)\n",
    "        scores['text'] = text\n",
    "        results.append(scores)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method converts a list of dicts into a DataFrame. It's left as a utility for the students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "def createDataFrameFromResults(results) :\n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is a convenience to change the source of our documents. We had intended to use MSN but that feed stopped working inexplicably so we have the BBC's World RSS feed as our default. There is also a feed of Japanese documents if you want to experiment with languages other than English. Keep in mind that different sources of documents may require different pre-processing to achieve better results. The parsing code may need to change slightly, the elements you need to discard before extracting the text may change slightly, but you have several options including class and id elements to filter in the tokenizeText methods above. Feel free to experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchFeed() :\n",
    "    #feedURL = \"http://rss.msn.com/en-us/money?feedoutput=rss\"\n",
    "    #feedURL = \"http://rss.asahi.com/rss/asahi/newsheadlines.rdf\"\n",
    "    feedURL = \"http://feeds.bbci.co.uk/news/world/rss.xml\"\n",
    "    feedFile = fetchUrlIfNecessary(feedURL)\n",
    "    feed = feedparser.parse(f\"file:///{feedFile}\")\n",
    "    return feed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "*INSTRUCTOR NOTE*: This is the first main exercise. It is intended to familiarize the students with the idea of processing a corpus, cleaning/pre-processsing the text, accumulating various results and then summarizing them. There are a variety of approaches you can take depending upon the time available and the background of the students.\n",
    "\n",
    "* You can simply walk through the solution and highlight the steps. Dive into the function definitions and explain where they can experiment and expect to see different results.\n",
    "* You can remove the definitions of the main functions that are called from this function and ask the students to fill one or more of them in. This is a way to constrain the exercise for scope and duration. The word frequency and sentiment analysis functions are the ones that are thought to be challenging but tractable.\n",
    "* In a longer course configuration or with advanced students you can have the students fill in the whole function. It's probably worthwhile to include at least pseudocode of the flow. Otherwise that might be too much to tackle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fetching: https://www.bbc.co.uk/news/world-latin-america-52682358\n-------------\nCoronavirus: Brazil's Bolsonaro sees second health minister quit:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound   -0.205367\nneg         0.129800\nneu         0.810533\npos         0.059533\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-asia-52673563\n-------------\nAfghan maternity ward attackers 'came to kill the mothers':, Comp: -0.8555, Pos: 0.0 , Neu: 0.455,  Neg: 0.545\ncompound   -0.197667\nneg         0.119186\nneu         0.811093\npos         0.069767\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-europe-52673225\n-------------\nCoronavirus: Surrogate babies stranded in Ukraine:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound    0.0\nneg         0.0\nneu         1.0\npos         0.0\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-europe-52685773\n-------------\nCoronavirus: Dutch singletons advised to seek ‘sex buddy’:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound    0.147157\nneg         0.030786\nneu         0.869571\npos         0.099714\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-us-canada-52685487\n-------------\nWhite House 'Operation Warp Speed' to look for Covid jab:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound    0.004565\nneg         0.049050\nneu         0.912750\npos         0.038200\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-us-canada-52675230\n-------------\nNFL's Deandre Baker and Quinton Dunbar charged with armed robbery:, Comp: -0.2023, Pos: 0.0 , Neu: 0.833,  Neg: 0.167\ncompound   -0.187300\nneg         0.106077\nneu         0.842462\npos         0.051462\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/business-52681414\n-------------\nUS targets Huawei with tighter chip export rules:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound   -0.159564\nneg         0.058409\nneu         0.913000\npos         0.028545\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-europe-52684575\n-------------\nCoronavirus: Moscow begins mass screening:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound    0.0\nneg         0.0\nneu         1.0\npos         0.0\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/business-52682411\n-------------\nUK now says no French quarantine exemption:, Comp: -0.296, Pos: 0.0 , Neu: 0.732,  Neg: 0.268\ncompound   -0.183244\nneg         0.063444\nneu         0.897667\npos         0.038889\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-us-canada-52684890\n-------------\nWorld War Two US Navy ship discovered in Pacific Ocean:, Comp: -0.5994, Pos: 0.0 , Neu: 0.698,  Neg: 0.302\ncompound   -0.5994\nneg         0.2620\nneu         0.7380\npos         0.0000\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/technology-52668164\n-------------\nGoogle spotlights more suspected Oculus VR gadget-scam ads:, Comp: -0.2944, Pos: 0.0 , Neu: 0.761,  Neg: 0.239\ncompound   -0.180764\nneg         0.106071\nneu         0.848536\npos         0.045500\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-asia-52628283\n-------------\nCoronavirus: How 'overreaction' made Vietnam a virus success:, Comp: 0.5719, Pos: 0.381 , Neu: 0.619,  Neg: 0.0\ncompound    0.001738\nneg         0.057750\nneu         0.880350\npos         0.061925\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-europe-52659481\n-------------\nCoronavirus: Is Putin rushing Russia out of lockdown?:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound   -0.181831\nneg         0.111194\nneu         0.832500\npos         0.056278\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-us-canada-52656756\n-------------\nCoronavirus: How the pandemic is relaxing US drinking laws:, Comp: 0.4939, Pos: 0.286 , Neu: 0.714,  Neg: 0.0\ncompound    0.067403\nneg         0.052119\nneu         0.863153\npos         0.084746\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-us-canada-52568184\n-------------\nAppalachian Trail: Covid postpones the great American adventure:, Comp: 0.6486, Pos: 0.474 , Neu: 0.37,  Neg: 0.156\ncompound    0.092323\nneg         0.078467\nneu         0.840800\npos         0.080733\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-asia-china-52651651\n-------------\nCoronavirus: Can China test all of Wuhan in only 10 days?:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound   -0.128164\nneg         0.039182\nneu         0.948909\npos         0.011909\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-europe-52663568\n-------------\nCoronavirus: Amsterdam trials 'Covid-safe' restaurant:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound    0.0\nneg         0.0\nneu         1.0\npos         0.0\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/uk-52637008\n-------------\nCoronavirus: How exposed is your job?:, Comp: -0.0772, Pos: 0.0 , Neu: 0.794,  Neg: 0.206\ncompound    0.149950\nneg         0.032708\nneu         0.920250\npos         0.047000\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-asia-india-52659520\n-------------\nAarogya Setu: Why India's Covid-19 contact tracing app is controversial:, Comp: -0.2023, Pos: 0.0 , Neu: 0.833,  Neg: 0.167\ncompound   -0.042356\nneg         0.092250\nneu         0.859062\npos         0.048687\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/science-environment-52648577\n-------------\nCoronavirus puts spotlight on landmark year for nature:, Comp: 0.0772, Pos: 0.157 , Neu: 0.843,  Neg: 0.0\ncompound    0.286791\nneg         0.035818\nneu         0.880909\npos         0.083182\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-europe-52637737\n-------------\nCoronavirus: France's boom city Toulouse shaken by pandemic:, Comp: -0.0772, Pos: 0.0 , Neu: 0.843,  Neg: 0.157\ncompound    0.050197\nneg         0.053686\nneu         0.891314\npos         0.055000\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/technology-52650576\n-------------\nCoronavirus: German contact-tracing app takes different path to NHS:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound    0.057300\nneg         0.053357\nneu         0.888929\npos         0.057857\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-latin-america-52632919\n-------------\nCoronavirus: The Colombian jail with 859 cases:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound   -0.369417\nneg         0.136250\nneu         0.819333\npos         0.044333\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-us-canada-52686786\n-------------\nTrump outlines plans for coronavirus vaccine:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound    0.0\nneg         0.0\nneu         1.0\npos         0.0\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/stories-52669638\n-------------\nHow the super-rich spent lockdown:, Comp: 0.0, Pos: 0.0 , Neu: 1.0,  Neg: 0.0\ncompound    0.0\nneg         0.0\nneu         1.0\npos         0.0\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-australia-52643540\n-------------\nAustralia Fires: Rescue dog Bear saves scores of koalas:, Comp: 0.5106, Pos: 0.292 , Neu: 0.708,  Neg: 0.0\ncompound    0.5106\nneg         0.0000\nneu         0.7520\npos         0.2480\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/uk-52649922\n-------------\nCoronavirus: New Zealand lockdown eased as businesses reopen:, Comp: 0.296, Pos: 0.239 , Neu: 0.761,  Neg: 0.0\ncompound    0.296\nneg         0.000\nneu         0.804\npos         0.196\ndtype: float64\n\nFetching: https://www.bbc.co.uk/news/world-us-canada-52668876\n-------------\nCoronavirus: Bamboo shortage forces Canadian zoo to return pandas to China:, Comp: -0.25, Pos: 0.0 , Neu: 0.833,  Neg: 0.167\ncompound   -0.250\nneg         0.143\nneu         0.857\npos         0.000\ndtype: float64\n\n[('``', 197), (\"''\", 159), (\"'s\", 130), ('people', 81), ('said', 70), ('coronavirus', 58), ('us', 53), ('says', 49), ('bbc', 44), ('app', 43), ('government', 42), ('news', 38), ('new', 37), ('one', 36), ('also', 35), ('would', 34), (\"n't\", 31), ('could', 29), ('city', 28), ('mr', 26), ('two', 25), ('cases', 24), ('may', 24), ('many', 24), ('made', 23), ('data', 23), ('health', 22), ('country', 21), ('alcohol', 21), ('told', 20), ('vietnam', 20), ('lockdown', 19), ('covid-19', 19), ('like', 19), ('used', 18), ('pandemic', 18), ('time', 18), ('toulouse', 18), ('virus', 17), ('use', 17), ('hospital', 17), ('vaccine', 17), ('even', 17), ('disease', 17), ('president', 16), ('world', 16), ('go', 16), ('huawei', 16), ('job', 15), ('open', 15)]\nFinished Processing Feed\n"
    }
   ],
   "source": [
    "# Retrieve a parsed version of our data source\n",
    "feed = fetchFeed()\n",
    "\n",
    "# Initialize a Counter to keep track of all the words\n",
    "# in our entire corpus (the RSS feed).\n",
    "corpusFreq = collections.Counter()\n",
    "\n",
    "# We iterate over each feed entry and will analyze it\n",
    "# in isolation. We will also aggregate word counts \n",
    "# across all of the documents.\n",
    "\n",
    "for post in feed.entries :\n",
    "    print(f\"Fetching: {post.link}\")\n",
    "\n",
    "    # We want to analyze the sentiment of the titles as\n",
    "    # distinct from the body of the articles. Are headlines\n",
    "    # spun to be more positive or negative?\n",
    "\n",
    "    results = analyzeSentiment(post.title)\n",
    "    print('-------------')\n",
    "    comp = results[0]['compound']\n",
    "    pos = results[0]['pos']\n",
    "    neu = results[0]['neu']\n",
    "    neg = results[0]['neg']\n",
    "\n",
    "    print(f\"{post.title}:, Comp: {comp}, Pos: {pos} , Neu: {neu},  Neg: {neg}\")\n",
    "\n",
    "    # We fetch a processed, cleaned up version of each entry. It's either\n",
    "    # locally cached or will be as part of this request so we don't hammer\n",
    "    # the source servers.\n",
    "\n",
    "    entryFile = fetchAndProcessUrlIfNecessary(post.link)\n",
    "\n",
    "    # Let's count the word frequencies for the file\n",
    "    docFreq = wordFrequency(entryFile, True)\n",
    "\n",
    "    # Update the corpus Counter with this document's contributions\n",
    "    corpusFreq.update(docFreq)\n",
    "\n",
    "    # Analyze the Sentiment of the article.\n",
    "    results = analyzeSentiment(entryFile)\n",
    "\n",
    "    # This is a somewhat gratuitous use of Pandas, but we just wanted to demonstrate\n",
    "    # how to connect these NLP techniques to tools you have learned to use in other\n",
    "    # Machine Learning courses. Here we simply convert the per sentence sentiment results\n",
    "    # into a Pandas DataFrame and then find the average score for the document (over all of\n",
    "    # the sentences).\n",
    "    df = createDataFrameFromResults(results)\n",
    "    print(df.mean())\n",
    "    print()\n",
    "\n",
    "# After we accumulate the results for each of the documents in the feed, we will\n",
    "# print out the 50 most common words across all of the documents in the feed.\n",
    "# Keep in mind that the quality of the results can be improved by removing common words,\n",
    "# lowercasing the tokens, removing punctuation, stop words, noisy HTML elements and \n",
    "# more. These results won't be perfect. As a homework item, try to improve the quality of\n",
    "# the word summaries by doing a better job cleaning up the source data.\n",
    "print(corpusFreq.most_common(50))\n",
    "print(\"Finished Processing Feed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "*INSTRUCTOR NOTE:* This is the next prominent exercise. The idea here is to do some analysis of the text documents using conventional tools such as the TfidVectorizer from Sci-Kit Learn. This is a remarkably straightforward analysis that doesn't involve the kind of math some students would learn in high school. The Text Vectorization process is lossy, however, so there are limits to the quality of the results. Still, this has been a useful advancement in the history of NLP. For the exercise, you can either just walk through it in the interest of time, or pick some piece of the process to create. The math to generate the correlation table is probably not something we can expect everyone to be familiar with, however, so you are strongly encouraged to at least give them that part of the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fetching: https://www.bbc.co.uk/news/world-latin-america-52682358\nFetching: https://www.bbc.co.uk/news/world-asia-52673563\nFetching: https://www.bbc.co.uk/news/world-europe-52673225\nFetching: https://www.bbc.co.uk/news/world-europe-52685773\nFetching: https://www.bbc.co.uk/news/world-us-canada-52685487\nFetching: https://www.bbc.co.uk/news/world-us-canada-52675230\nFetching: https://www.bbc.co.uk/news/business-52681414\nFetching: https://www.bbc.co.uk/news/world-europe-52684575\nFetching: https://www.bbc.co.uk/news/business-52682411\nFetching: https://www.bbc.co.uk/news/world-us-canada-52684890\nFetching: https://www.bbc.co.uk/news/technology-52668164\nFetching: https://www.bbc.co.uk/news/world-asia-52628283\nFetching: https://www.bbc.co.uk/news/world-europe-52659481\nFetching: https://www.bbc.co.uk/news/world-us-canada-52656756\nFetching: https://www.bbc.co.uk/news/world-us-canada-52568184\nFetching: https://www.bbc.co.uk/news/world-asia-china-52651651\nFetching: https://www.bbc.co.uk/news/world-europe-52663568\nFetching: https://www.bbc.co.uk/news/uk-52637008\nFetching: https://www.bbc.co.uk/news/world-asia-india-52659520\nFetching: https://www.bbc.co.uk/news/science-environment-52648577\nFetching: https://www.bbc.co.uk/news/world-europe-52637737\nFetching: https://www.bbc.co.uk/news/technology-52650576\nFetching: https://www.bbc.co.uk/news/world-latin-america-52632919\nFetching: https://www.bbc.co.uk/news/world-us-canada-52686786\nFetching: https://www.bbc.co.uk/news/stories-52669638\nFetching: https://www.bbc.co.uk/news/world-australia-52643540\nFetching: https://www.bbc.co.uk/news/uk-52649922\nFetching: https://www.bbc.co.uk/news/world-us-canada-52668876\n28\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                       Score\nCoronavirus: Brazil's Bolsonaro sees second hea...  1.000000\nAppalachian Trail: Covid postpones the great Am...  0.503644\nCoronavirus: Is Putin rushing Russia out of loc...  0.482085\nAfghan maternity ward attackers 'came to kill t...  0.467555\nCoronavirus: France's boom city Toulouse shaken...  0.460387\nCoronavirus: How 'overreaction' made Vietnam a ...  0.458224\nAarogya Setu: Why India's Covid-19 contact trac...  0.422059\nCoronavirus: How the pandemic is relaxing US dr...  0.420095\nCoronavirus: Dutch singletons advised to seek ‘...  0.407525\nCoronavirus: Can China test all of Wuhan in onl...  0.387295\nGoogle spotlights more suspected Oculus VR gadg...  0.383177\nWhite House 'Operation Warp Speed' to look for ...  0.378298\nCoronavirus: German contact-tracing app takes d...  0.377826\nUK now says no French quarantine exemption          0.375389\nCoronavirus: How exposed is your job?               0.375188\nCoronavirus puts spotlight on landmark year for...  0.359641\nCoronavirus: The Colombian jail with 859 cases      0.351060\nUS targets Huawei with tighter chip export rules    0.340270\nNFL's Deandre Baker and Quinton Dunbar charged ...  0.289723\nHow the super-rich spent lockdown                   0.113389\nCoronavirus: Bamboo shortage forces Canadian zo...  0.072367\nCoronavirus: New Zealand lockdown eased as busi...  0.053261\nTrump outlines plans for coronavirus vaccine        0.046681\nCoronavirus: Surrogate babies stranded in Ukraine   0.030543\nWorld War Two US Navy ship discovered in Pacifi...  0.028489\nAustralia Fires: Rescue dog Bear saves scores o...  0.024584\nCoronavirus: Amsterdam trials 'Covid-safe' rest...  0.018869\nCoronavirus: Moscow begins mass screening           0.015658",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Coronavirus: Brazil's Bolsonaro sees second health minister quit</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>Appalachian Trail: Covid postpones the great American adventure</th>\n      <td>0.503644</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: Is Putin rushing Russia out of lockdown?</th>\n      <td>0.482085</td>\n    </tr>\n    <tr>\n      <th>Afghan maternity ward attackers 'came to kill the mothers'</th>\n      <td>0.467555</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: France's boom city Toulouse shaken by pandemic</th>\n      <td>0.460387</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: How 'overreaction' made Vietnam a virus success</th>\n      <td>0.458224</td>\n    </tr>\n    <tr>\n      <th>Aarogya Setu: Why India's Covid-19 contact tracing app is controversial</th>\n      <td>0.422059</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: How the pandemic is relaxing US drinking laws</th>\n      <td>0.420095</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: Dutch singletons advised to seek ‘sex buddy’</th>\n      <td>0.407525</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: Can China test all of Wuhan in only 10 days?</th>\n      <td>0.387295</td>\n    </tr>\n    <tr>\n      <th>Google spotlights more suspected Oculus VR gadget-scam ads</th>\n      <td>0.383177</td>\n    </tr>\n    <tr>\n      <th>White House 'Operation Warp Speed' to look for Covid jab</th>\n      <td>0.378298</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: German contact-tracing app takes different path to NHS</th>\n      <td>0.377826</td>\n    </tr>\n    <tr>\n      <th>UK now says no French quarantine exemption</th>\n      <td>0.375389</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: How exposed is your job?</th>\n      <td>0.375188</td>\n    </tr>\n    <tr>\n      <th>Coronavirus puts spotlight on landmark year for nature</th>\n      <td>0.359641</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: The Colombian jail with 859 cases</th>\n      <td>0.351060</td>\n    </tr>\n    <tr>\n      <th>US targets Huawei with tighter chip export rules</th>\n      <td>0.340270</td>\n    </tr>\n    <tr>\n      <th>NFL's Deandre Baker and Quinton Dunbar charged with armed robbery</th>\n      <td>0.289723</td>\n    </tr>\n    <tr>\n      <th>How the super-rich spent lockdown</th>\n      <td>0.113389</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: Bamboo shortage forces Canadian zoo to return pandas to China</th>\n      <td>0.072367</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: New Zealand lockdown eased as businesses reopen</th>\n      <td>0.053261</td>\n    </tr>\n    <tr>\n      <th>Trump outlines plans for coronavirus vaccine</th>\n      <td>0.046681</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: Surrogate babies stranded in Ukraine</th>\n      <td>0.030543</td>\n    </tr>\n    <tr>\n      <th>World War Two US Navy ship discovered in Pacific Ocean</th>\n      <td>0.028489</td>\n    </tr>\n    <tr>\n      <th>Australia Fires: Rescue dog Bear saves scores of koalas</th>\n      <td>0.024584</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: Amsterdam trials 'Covid-safe' restaurant</th>\n      <td>0.018869</td>\n    </tr>\n    <tr>\n      <th>Coronavirus: Moscow begins mass screening</th>\n      <td>0.015658</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 350
    }
   ],
   "source": [
    "\n",
    "# Retrieve a parsed version of our data source\n",
    "feed = fetchFeed()\n",
    "\n",
    "# We are going to accumulate the processed forms of the input \n",
    "# documents and their headlines.\n",
    "documents = []\n",
    "headlines = []\n",
    "\n",
    "# Iterate over all of the input documents to grab the document\n",
    "# text and the associated headlines.\n",
    "for post in feed.entries :\n",
    "    print(f\"Fetching: {post.link}\")\n",
    "\n",
    "    entryFile = fetchAndProcessUrlIfNecessary(post.link)\n",
    "    documents.append(''.join(entryFile))\n",
    "    headlines.append(post.title)\n",
    "\n",
    "# The TF-IDF Vectorizer class is from Sci-Kit Learn and allows us\n",
    "# to vectorize the corpus based upon this common NLP tool:\n",
    "# https://en.wikipedia.org/wiki/Tf–idf\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# We transform the documents into a collection of vectors and \n",
    "# compare them to each other. An explanation of what is going\n",
    "# on can be found here: https://medium.com/@odysseus1337/document-class-comparison-with-tf-idf-python-1b4860b9345b\n",
    "vecs = tfidf.fit_transform(documents)\n",
    "matrix = ((vecs * vecs.T).A)\n",
    "\n",
    "# We convert the first row into a data frame. Each value represents the first document with its\n",
    "# correlation against the other documents based upon the TF-IDF vectorization. We index the DataFrame\n",
    "# by the corresponding headlines so its easier to see which articles are more alike which other ones.\n",
    "df = pd.DataFrame(matrix[0,:], index=headlines, columns=[\"Score\"])\n",
    "df.sort_values(by=\"Score\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to switch from using local tools running solely on your machines to taking advantage of services, computational power and pre-trained models. We are going to rely on the Azure Cognitive Services Text Analytics tools. This does require you to sign up for a free account. There should be no charges.\n",
    "\n",
    "After you register, you will need to locate the TextAnalytics page on the Azure Portal to discover your user key and endpoint. Please paste those in below and uncomment the lines for the key and endpoint variable definitions. Do not check in keys like this to a public repository!\n",
    "\n",
    "Because the Azure services require authentication, the interaction is fairly complicated. To make this easier to use, Microsoft provides a series of language-specific clients that will shield you from this complexity if you use one. We are going to uses the Python Client but there are libraries for Java and C# too.\n",
    "\n",
    "To install the python client, you will need to run the following command:\n",
    "\n",
    "```pip install upgrade azure-cognitiveservices-language-textanalytics```\n",
    "\n",
    "You just need to run this cell to make the client set up available for our use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = ADD_YOUR_KEY_HERE\n",
    "# endpoint = ADD_YOUR_ENDPOINT_URL_HERE\n",
    "\n",
    "import os\n",
    "from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "def authenticateClient():\n",
    "    credentials = CognitiveServicesCredentials(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint, credentials=credentials)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticateClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*INSTRUCTOR NOTE*: The final large exercise introduces the Azure Cognitive Services Text Analytics Services. While it is cool that open source tools such as NLTK and Sci-Kit Learn can be used by anyone to build up various sophisticated machine learning models for processing text, there are some operational considerations that should be considered. You may have more data to process than you have machines to use. Azure Cloud services can help spin up capacity as needed. More crucially, you may not have enough data to build sophisticated models. The Text Analytics Services let us leverage pre-trained models through convenient APIs with a capacity for growth in the computational demands. There is also nothing stopping you from mixing and matching these backend API services with locally running NLTK and Sci-Kit code.\n",
    "\n",
    "This exercise reuses our existing infrastructure, but the Azure Text Analytics services expect the data to be structured a little differently. We build up a keyed representation of the documents and accumulate more details as we use different services. For example, one service will tell us the language of the document (e.g. English, French, Japanese). We will capture these details and pass them on. Upstream systems might need to select different lexicons, parsing rules, stop word lists, etc. depending on the document's language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fetching: https://www.bbc.co.uk/news/world-latin-america-52682358\nFetching: https://www.bbc.co.uk/news/world-asia-52673563\nFetching: https://www.bbc.co.uk/news/world-europe-52673225\nFetching: https://www.bbc.co.uk/news/world-europe-52685773\nFetching: https://www.bbc.co.uk/news/world-us-canada-52685487\nFetching: https://www.bbc.co.uk/news/world-us-canada-52675230\nFetching: https://www.bbc.co.uk/news/business-52681414\nFetching: https://www.bbc.co.uk/news/world-europe-52684575\nFetching: https://www.bbc.co.uk/news/business-52682411\nFetching: https://www.bbc.co.uk/news/world-us-canada-52684890\nFetching: https://www.bbc.co.uk/news/technology-52668164\nFetching: https://www.bbc.co.uk/news/world-asia-52628283\nFetching: https://www.bbc.co.uk/news/world-europe-52659481\nFetching: https://www.bbc.co.uk/news/world-us-canada-52656756\nFetching: https://www.bbc.co.uk/news/world-us-canada-52568184\nFetching: https://www.bbc.co.uk/news/world-asia-china-52651651\nFetching: https://www.bbc.co.uk/news/world-europe-52663568\nFetching: https://www.bbc.co.uk/news/uk-52637008\nFetching: https://www.bbc.co.uk/news/world-asia-india-52659520\nFetching: https://www.bbc.co.uk/news/science-environment-52648577\nFetching: https://www.bbc.co.uk/news/world-europe-52637737\nFetching: https://www.bbc.co.uk/news/technology-52650576\nFetching: https://www.bbc.co.uk/news/world-latin-america-52632919\nFetching: https://www.bbc.co.uk/news/world-us-canada-52686786\nFetching: https://www.bbc.co.uk/news/stories-52669638\nFetching: https://www.bbc.co.uk/news/world-australia-52643540\nFetching: https://www.bbc.co.uk/news/uk-52649922\nFetching: https://www.bbc.co.uk/news/world-us-canada-52668876\n------------------------------------------\n"
    }
   ],
   "source": [
    "# Retrieve a parsed version of our data source\n",
    "feed = fetchFeed()\n",
    "\n",
    "documents = []\n",
    "i = 0\n",
    "\n",
    "for post in feed.entries :\n",
    "    print(f\"Fetching: {post.link}\")\n",
    "\n",
    "    entryFile = fetchAndProcessUrlIfNecessary(post.link)\n",
    "\n",
    "    # Some of the Cognitive Services have size limits on documents. \n",
    "    # You can clearly break them into chunks and process them separately,\n",
    "    # but we are just going to grab a chunk below the threshold so we\n",
    "    # don't error out later.\n",
    "    if len(entryFile) > 5000 :\n",
    "        entryFile = entryFile[0:5000] \n",
    "\n",
    "    documents.append({'title' : post.title, 'text' : entryFile, \"id\" : i})\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# Start processing the documents with the Azure Services\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# This service detects the language of the documents and returns the results\n",
    "# with the associated id so we can keep track. It is important to note that\n",
    "# this local method call triggers calls to backend services.\n",
    "\n",
    "response = client.detect_language(documents=documents)\n",
    "\n",
    "# We iterate over the results per document and update the associated document metadata.\n",
    "for document in response.documents:\n",
    "   documents[int(document.id)]['language'] = document.detected_languages[0].iso6391_name\n",
    "\n",
    "# This service issues an aggregate sentiment score between 0.0 (negative) and 1.0 (positive).\n",
    "# 0.5 represents a neutral sentiment. It is important to note that this local method call\n",
    "# triggers calls to backend services.\n",
    "\n",
    "response = client.sentiment(documents=documents)\n",
    "\n",
    "# We iterate over the results per document and update the associated document metadata.\n",
    "for document in response.documents:\n",
    "    documents[int(document.id)]['sentiment'] = document.score\n",
    "\n",
    "# One of the other useful services we can call on our documents involve extracting \n",
    "# key phrases from the text. This could be useful for categorizing and clustering\n",
    "# documents.\n",
    "response = client.key_phrases(documents=documents)\n",
    "\n",
    "# We iterate over the results per document and update the associated document metadata\n",
    "for document in response.documents:\n",
    "   documents[int(document.id)]['keyphrases'] = document.key_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is just a useful placeholder for exploring results. You can see what details we have learned per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'title': \"Afghan maternity ward attackers 'came to kill the mothers'\", 'text': 'Afghan maternity ward attackers \\'came to kill the mothers\\' - BBC News\\nThe cold-blooded murders of 24 women, children and babies at a hospital in the Afghan capital was horrific enough.But as Frederic Bonnot made his way through the bullet-riddled maternity unit, he realised something more.Warning: Readers may find details in this story upsettingThe attackers had walked straight past a number of other wards, all closer to the entrance of Kabul\\'s Dasht-e-Barchi hospital, and made straight for the maternity unit.To him, it meant one thing: this was no mistake.\\n\"What I saw in the maternity demonstrates it was a systematic shooting of the mothers,\" Bonnot, Medicin Sans Frontiere\\'s (MSF) Head of Programmes in Afghanistan, said. \"They went through the rooms in the maternity, shooting women in their beds. It was methodical.\"They came to kill the mothers.\" \\'Beyond words\\'Amina was just two hours old when the attack started.The little girl was the third child for Bibi Nazia and her husband, Rafiullah. Back at home, they already had a girl and a boy.Nazia had gone to the hospital with her mother, and Amina was born at 08:00.It should have been a day of celebration for Rafiullah. But at 10:00, the attack began. Explosions were heard by people outside the hospital complex. Those with family and friends inside rushed to the scene - including Rafiullah.\"He ran from side to side. But he couldn\\'t do anything, no one allowed him to go inside,\" his cousin Hamidullah Hamidi told BBC Pashto.\\nNineteen babies are being cared for at the Ataturk Children\\'s Hospital\\nInside the walls of the hospital, three gunmen were moving through the 55-bed maternity unit, which has been run by MSF since 2014.A total of 26 mothers and mothers-to-be were inside at the time. Ten managed to flee to safe rooms; the other 16 - including Bibi Nazia and Amina - were not so lucky.Three of the 16 mothers were shot and killed in the delivery room, along with their unborn babies. Bibi Nazia was among the other eight mothers killed; little Amina was shot in the legs. Five more were wounded. Two young boys were also killed in the carnage, along with a midwife.One woman, named only as Khadija, told Reuters news agency how one of the gunmen had pointed his weapon at her, before turning it on two other people.\\nThe gun battle between the militants and Afghan security forces went on for four hours as many of the maternity unit\\'s patients and staff took cover. The gunmen were all killed.One baby born was at the hospital during those hours. A midwife, speaking on condition of anonymity, revealed to the AFP news agency how the woman sheltering with her in the safe room gave birth while trying not to make a sound.\"We helped her with our bare hands, we had nothing else in the room except some toilet paper and our scarves,\" the midwife said.\"When the baby was born, we cut the umbilical cord using our hands. We used our headscarves to wrap the baby and the mother.\"\\nFrederic Bonnot described how those inside the safe rooms could only listen as explosions and gunfire ricocheted around the building.By the time they were finished, he said, they found \"walls sprayed with bullets, blood on the floors in the rooms, vehicles burnt out and windows shot through\". \"It\\'s shocking,\" said Bonnot. \"We know this area has suffered attacks in the past, but no one could believe they would attack a maternity unit.\"This country is sadly used to seeing horrific events,\" he added. \"But what happened Tuesday is beyond words.\"\\nIt is true Afghanistan is used to seeing violence most of the rest of the world cannot imagine. A BBC investigation last year found that, on average, 74 men, women and children were killed every day throughout the month of August.A fifth of them were civilians, the BBC found.Within hours of the Dasht-e-Barchi hospital attack, a suicide bomber had killed at least another 32 people attending a policeman\\'s funeral in eastern Nangarhar province. Meanwhile, in northern Balkh province, at least 10 people were killed and many others injured in an air strike by US forces, reports said. Residents and the Taliban claimed the victims were all civilians, but defence officials said all those killed were militants.Two days later, at least five civilians were killed in an attack in Paktia province\\'s capital, Gardez. The killings come at a time when, technically, the country should be moving towards peace talks.The US signed a deal with the Taliban militant group in February, hoping to end an 18-year conflict which has ravaged Afghanistan.\\nIs peace with the Taliban possible?\\nAs part of the agreement, the Taliban agreed to enter talks with the Afghan government, something they had so far refused to do.But as violence continues to shatter the country, any hope for a peace deal looks to be on delicate ground: after the hospital attack, President Ashraf Ghani ordered the resumption of offensive operations against the Taliban and other groups.He accused the militants of ignoring repeated calls for a reduction in violence.No group has said it carried out the maternity hospital massacre. The Taliban claimed the Gardez city blast, while the Islamic State (IS) group took responsibility for the funeral attack.However, US envoy Zalmay Khalilzad has blamed IS for the hospital attack, saying the jihadists want to undermine recent peace efforts and fan a sectarian war.\\nHis words are of little use to those caught up in Tuesday\\'s attack - to the grandmother who had to pick up her fatally-injured new-born grandson from the floor, to the murdered midwife who had worked for MSF for six years, or to the survivors who listened helplessly as the assault unfolded.The babies were moved to the Ataturk hospital in Kabul, the families who had gathered outside the maternity unit following - desperately waiting to see if their child\\'s name was called.In the confusion, women appeared to offer to adopt the babies, sparking at least . Officials hoped to reunite all the children by Friday.Little Amina is now recovering in a different hospital. She has already had one round of surgery, with at least two more to go. The doctors hope they can save the leg shattered by a bullet, but they don\\'t know yet.The family fear for her future if they do have to amputate - life is not likely to be easy for a disabled girl in Afghanistan.But right now, they are just trying to understand why, as the country supposedly moves towards peace, this happened.\"People have been shouting for 19 or 20 years not to commit atrocities,\" Hamidullah Hamidi said. \"But they have not stopped. These people will not stop committing atrocities.\"', 'id': 1}\n"
    }
   ],
   "source": [
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "The following exercise invokes behavior from the Azure Cognitive Services Text Analytics client to extract entity references from our documents. As we iterate over the response from the service (in the first line), we add the entity references to each of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This service extracts named entities from the document including names, type and subtype information.\n",
    "# It is important to note that this local method call triggers calls to backend services.\n",
    "\n",
    "response = client.entities(documents=documents)\n",
    "\n",
    "# TODO: Handle errors\n",
    "\n",
    "for document in response.documents:\n",
    "    entities = []\n",
    "\n",
    "    for entity in document.entities:\n",
    "        # It is completely arbitrary that we are filtering out these entity types. They are perfectly valid\n",
    "        # types and are likely to be of interest in processing financial news data.\n",
    "        if entity.type != 'Quantity' and entity.type != 'DateTime':\n",
    "            entities.append({'name' : entity.name, 'type' : entity.type, 'subtype' : entity.sub_type })\n",
    "\n",
    "    documents[int(document.id)]['entities'] = entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is a convenience for keeping track of sentiment references and the associated headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addReference(refs, key, sentiment, text):\n",
    "   currentRefs = refs.get(key, {'sentiment' : [], 'headline' : []})\n",
    "   currentRefs['sentiment'].append(document['sentiment'])\n",
    "   currentRefs['headline'].append(document['title'])\n",
    "   refs[key] = currentRefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions are convenience functions to calculate the average sentiment score and to summarize the details of a specific set of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageSentimentValue(ratings) :\n",
    "    return np.array(ratings).mean()\n",
    "\n",
    "def entitySummary(entityType, refs, polarity):\n",
    "    print(f\"{entityType} with the most {polarity} coverage:\")\n",
    "\n",
    "    for entity in refs.keys() :\n",
    "        print('---------------')\n",
    "        print(f\"Name: {entity}\")\n",
    "        avg = averageSentimentValue(refs[entity]['sentiment'])\n",
    "        print(f\" .  Average sentiment analysis: {avg}\")\n",
    "        headlines = refs[entity]['headline']\n",
    "        print(\" .  Headlines:\")\n",
    "        for h in headlines:\n",
    "            print(f\"      {h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is going to add up the positive and negative entity references based on the articles they were mentioned in. The Text Analytics sentiment analysis service returns a value of 0.0 (very negative) to 1.0 (very positive) as a range. Neutral would register as 0.5. We picked >= 0.6 as the threshold for a positive mention and <= 0.4 for a negative mention. Feel free to play with those thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "28\nAdding - org reference for BBC News\nDONE ADDING REF\nAdding - org reference for Google\nDONE ADDING REF\nAdding - org reference for Oculus VR\nDONE ADDING REF\nAdding - org reference for BBC\nDONE ADDING REF\nAdding - org reference for BBC News\nFresh\nDONE ADDING REF\nAdding - org reference for Techziox.com\nDONE ADDING REF\nAdding - org reference for Shopzeal.co.\nDONE ADDING REF\nAdding - org reference for CVV\nDONE ADDING REF\nAdding - org reference for Stripe (company)\nDONE ADDING REF\nAdding - org reference for Techziox\nDONE ADDING REF\nAdding - org reference for Wordpress\nDONE ADDING REF\nAdding - org reference for HM Revenue and Customs\nDONE ADDING REF\nAdding - person reference for Nicky Jones\nAdding - org reference for Google Shopping\nDONE ADDING REF\nAdding + org reference for BBC\nAdding - org reference for BBC\nDONE ADDING REF\nAdding - person reference for Miguel Ángel Rodríguez\nAdding - person reference for Rodríguez\nAdding - person reference for Lissette Cervantes\nAdding - person reference for Cervantes\nAdding + org reference for BBC News\nAdding + org reference for BBC\nAdding - org reference for BBC News\nDONE ADDING REF\n"
    }
   ],
   "source": [
    "entityCount = {}\n",
    "entityTypeCount = {}\n",
    "posPersonRefs = {}\n",
    "negPersonRefs = {}\n",
    "posOrgRefs = {}\n",
    "negOrgRefs = {}\n",
    "\n",
    "# For each processed document, we look at the entities and their types as \n",
    "# identified by the Cognitive Services Text Analytics service. We add up\n",
    "# all of the Person and Organization entities that are referenced. We\n",
    "# limited it to those for this discussion of processing financial news as data.\n",
    "# There are certainly other entity types mentioned and you should feel free to\n",
    "# modify the following code to track references for them too.\n",
    "for document in documents:\n",
    "    for entity in document['entities']:\n",
    "        # The previous steps captured entity names and types per document\n",
    "        name = entity['name']\n",
    "        t = entity['type']\n",
    "\n",
    "        # We increment the references we see for\n",
    "        # each named entity and its type.\n",
    "        count = entityCount.get(name, 0)\n",
    "        entityCount[name] = count + 1\n",
    "        count = entityTypeCount.get(t, 0)\n",
    "        entityTypeCount[t] = count + 1\n",
    "\n",
    "        # This is an arbitrary threshold as mentioned above. Feel free to\n",
    "        # experiment with values to change the definition of a positive or\n",
    "        # negative reference.\n",
    "        if document['sentiment'] >= 0.6:\n",
    "            # Add the positive references\n",
    "            if t == 'Person' :\n",
    "                print(f\"Adding + person reference for {name}\")\n",
    "                addReference(posPersonRefs, name, document['sentiment'], document['title'])\n",
    "            elif t == 'Organization' :\n",
    "                print(f\"Adding + org reference for {name}\")\n",
    "                addReference(posOrgRefs, name, document['sentiment'], document['title'])\n",
    "\n",
    "        elif document['sentiment'] <= 0.4:\n",
    "            # Add the negative references\n",
    "            if t == 'Person' :\n",
    "                print(f\"Adding - person reference for {name}\")\n",
    "                addReference(negPersonRefs, name, document['sentiment'], document['title'])\n",
    "            elif t == 'Organization' :\n",
    "                print(f\"Adding - org reference for {name}\")\n",
    "                addReference(negOrgRefs, name, document['sentiment'], document['title'])\n",
    "                print(\"DONE ADDING REF\")\n",
    "\n",
    "            elif t == 'Organization' :\n",
    "                print(f\"Adding + org reference for {name}\")\n",
    "                addReference(posOrgRefs, name, document['sentiment'], document['title'])\n",
    "\n",
    "        elif document['sentiment'] <= 0.4:\n",
    "            # Add the negative references\n",
    "            if t == 'Person' :\n",
    "                print(f\"Adding - person reference for {name}\")\n",
    "                addReference(negPersonRefs, name, document['sentiment'], document['title'])\n",
    "            elif t == 'Organization' :\n",
    "                print(f\"Adding - org reference for {name}\")\n",
    "                addReference(negOrgRefs, name, document['sentiment'], document['title'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a convenience function to print out the top positive and negative references we saw across the corpus for People and Organizations. If you want to capture metrics about other entity types, you will most likely have to modify that part of the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "People with the most positive coverage:\nPeople with the most negative coverage:\n---------------\nName: Nicky Jones\n .  Average sentiment analysis: 0.058806538581848145\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n---------------\nName: Miguel Ángel Rodríguez\n .  Average sentiment analysis: 0.09739166498184204\n .  Headlines:\n      Coronavirus: The Colombian jail with 859 cases\n---------------\nName: Rodríguez\n .  Average sentiment analysis: 0.09739166498184204\n .  Headlines:\n      Coronavirus: The Colombian jail with 859 cases\n---------------\nName: Lissette Cervantes\n .  Average sentiment analysis: 0.09739166498184204\n .  Headlines:\n      Coronavirus: The Colombian jail with 859 cases\n---------------\nName: Cervantes\n .  Average sentiment analysis: 0.09739166498184204\n .  Headlines:\n      Coronavirus: The Colombian jail with 859 cases\nOrganizations with the most positive coverage:\n---------------\nName: BBC\n .  Average sentiment analysis: 0.926816463470459\n .  Headlines:\n      Coronavirus: Amsterdam trials 'Covid-safe' restaurant\n      Coronavirus: New Zealand lockdown eased as businesses reopen\n---------------\nName: BBC News\n .  Average sentiment analysis: 0.8754132986068726\n .  Headlines:\n      How the super-rich spent lockdown\nOrganizations with the most negative coverage:\n---------------\nName: BBC News\n .  Average sentiment analysis: 0.1877424716949463\n .  Headlines:\n      Coronavirus: Surrogate babies stranded in Ukraine\n      Coronavirus: Bamboo shortage forces Canadian zoo to return pandas to China\n---------------\nName: Google\n .  Average sentiment analysis: 0.058806538581848145\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n---------------\nName: Oculus VR\n .  Average sentiment analysis: 0.058806538581848145\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n---------------\nName: BBC\n .  Average sentiment analysis: 0.07809910178184509\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n      Coronavirus: The Colombian jail with 859 cases\n---------------\nName: BBC News\nFresh\n .  Average sentiment analysis: 0.058806538581848145\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n---------------\nName: Techziox.com\n .  Average sentiment analysis: 0.058806538581848145\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n---------------\nName: Shopzeal.co.\n .  Average sentiment analysis: 0.058806538581848145\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n---------------\nName: CVV\n .  Average sentiment analysis: 0.058806538581848145\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n---------------\nName: Stripe (company)\n .  Average sentiment analysis: 0.058806538581848145\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n---------------\nName: Techziox\n .  Average sentiment analysis: 0.058806538581848145\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n---------------\nName: Wordpress\n .  Average sentiment analysis: 0.058806538581848145\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n---------------\nName: HM Revenue and Customs\n .  Average sentiment analysis: 0.058806538581848145\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n---------------\nName: Google Shopping\n .  Average sentiment analysis: 0.058806538581848145\n .  Headlines:\n      Google spotlights more suspected Oculus VR gadget-scam ads\n"
    }
   ],
   "source": [
    "entitySummary('People', posPersonRefs, 'positive')\n",
    "entitySummary('People', negPersonRefs, 'negative')\n",
    "entitySummary('Organizations', posOrgRefs, 'positive')\n",
    "entitySummary('Organizations', negOrgRefs, 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}